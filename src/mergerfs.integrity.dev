#!/usr/bin/env python3

# Copyright (c) 2016, Antonio SJ Musumeci <trapexit@spawn.link>
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import argparse
import ctypes
import errno
import fnmatch
import io
import os
import shlex
import subprocess
import sys
import time
import signal
import fcntl
import json
import pickle
import csv
import gzip
import hashlib
from datetime import datetime
# from rich.pretty import pprint
# from rich import print as cprint
from rich.console import Console
console = Console()

_libc = ctypes.CDLL("libc.so.6",use_errno=True)
_lgetxattr = _libc.lgetxattr
_lgetxattr.argtypes = [ctypes.c_char_p,ctypes.c_char_p,ctypes.c_void_p,ctypes.c_size_t]
def lgetxattr(path,name):
    if type(path) == str:
        path = path.encode(errors='backslashreplace')
    if type(name) == str:
        name = name.encode(errors='backslashreplace')
    length = 64
    while True:
        buf = ctypes.create_string_buffer(length)
        res = _lgetxattr(path,name,buf,ctypes.c_size_t(length))
        if res >= 0:
            return buf.raw[0:res].decode(errors='backslashreplace')
        else:
            err = ctypes.get_errno()
            if err == errno.ERANGE:
                length *= 2
            elif err == errno.ENODATA:
                return None
            else:
                raise IOError(err,os.strerror(err),path)


def ismergerfs(path):
    try:
        lgetxattr(path,'user.mergerfs.basepath')
        return True
    except IOError as e:
        return False


def mergerfs_control_file(basedir):
    if basedir == '/':
        return None
    ctrlfile = os.path.join(basedir,'.mergerfs')
    if os.path.exists(ctrlfile):
        return ctrlfile
    basedir = os.path.dirname(basedir)
    return mergerfs_control_file(basedir)


def mergerfs_branches(ctrlfile):
    branches = lgetxattr(ctrlfile,'user.mergerfs.srcmounts')
    branches = branches.split(':')
    return branches


def match(filename,matches):
    for match in matches:
        if fnmatch.fnmatch(filename,match):
            return True
    return False


def execute_cmd(args):
    # print(args)
    return subprocess.call(args)


def print_args(args):
    quoted = [shlex.quote(arg) for arg in args]
    print(' '.join(quoted))


def build_copy_file(src,tgt,rel):
    srcpath = os.path.join(src,'./',rel)
    tgtpath = tgt + '/'
    return ['rsync',
            '-avHAXWE',
            '--numeric-ids',
            #'--progress',
            '--relative',
            srcpath,
            tgtpath]
'''
--archive, -a            archive mode is -rlptgoD (no -A,-X,-U,-N,-H)
    --recursive, -r          recurse into directories
    --links, -l              copy symlinks as symlinks
    --perms, -p              preserve permissions
    --times, -t              preserve modification times
    --group, -g              preserve group
    --owner, -o              preserve owner (super-user only)
    -D                       same as --devices --specials
        --devices                preserve device files (super-user only)
        --specials               preserve special files
--numeric-ids            don't map uid/gid values by user/group name
--relative, -R           use relative path names

--atimes, -U             preserve access (use) times
--open-noatime           avoid changing the atime on opened files
--crtimes, -N            preserve create times (newness)
'''


def build_branches_freespace(branches, caches):
    rv = dict()
    rc = dict()
    for branch in branches:
        st = os.statvfs(branch)
        if match(branch, caches):
            rc[branch] = st.f_bavail * st.f_frsize
        else:
            rv[branch] = st.f_bavail * st.f_frsize
    return (rv, rc)


def print_help():
    help = \
'''
usage: mergerfs.dup [<options>] <dir>

Duplicate files & directories across multiple drives in a pool.
Will print out commands for inspection and out of band use.

positional arguments:
  dir                    starting directory

optional arguments:
  -c, --count=           Number of copies to create. (default: 2)
  -d, --dup=             Which file (if more than one exists) to choose to
                         duplicate. Each one falls back to `mergerfs` if
                         all files have the same value. (default: newest)
                         * newest   : file with largest mtime
                         * oldest   : file with smallest mtime
                         * smallest : file with smallest size
                         * largest  : file with largest size
                         * mergerfs : file chosen by mergerfs' getattr
  -p, --prune            Remove files above `count`. Without this enabled
                         it will update all existing files.
  -e, --execute          Execute `rsync` and `rm` commands. Not just
                         print them.
  -I, --include=         fnmatch compatible filter to include files, directories.
                         Can be used multiple times.
  -E, --exclude=         fnmatch compatible filter to exclude files, directories..
                         Can be used multiple times.
  -C, --cache=           Cache disk. Does not count towards 'count' copies.

  -X, --exclude-cache    fnmatch compatible fitler to exclude from caching.
'''
    print(help)


def buildargparser():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('dir',
                        type=str,
                        nargs='?',
                        default=None)
    parser.add_argument('-c','--count',
                        dest='count',
                        type=int,
                        default=2)
    parser.add_argument('-p','--prune',
                        dest='prune',
                        action='store_true')
    parser.add_argument('-d','--dup',
                        choices=['newest','oldest',
                                 'smallest','largest',
                                 'mergerfs'],
                        default='mergerfs')
    parser.add_argument('-e','--execute',
                        dest='execute',
                        action='store_true')
    parser.add_argument('-I','--include',
                        dest='include',
                        type=str,
                        action='append',
                        default=[])
    parser.add_argument('-E','--exclude',
                        dest='exclude',
                        type=str,
                        action='append',
                        default=[])
    parser.add_argument('-X','--exclude-cache',
                        dest='excludecache',
                        type=str,
                        action='append',
                        default=[])
    parser.add_argument('-C','--cache',
                        dest='cache',
                        type=str,
                        action='append',
                        default=[])
    parser.add_argument('-D','--diff',
                        dest='diff',
                        type=str,
                        # action='append',
                        default="mtime,size,uid,gid,mode")
                        # default="uid,gid,mode")

    parser.add_argument('-h','--help',
                        action='store_true')

    return parser


def xattr_basepath(fullpath):
    return lgetxattr(fullpath,'user.mergerfs.basepath')


def xattr_allpaths(fullpath):
    return lgetxattr(fullpath,'user.mergerfs.allpaths')


def xattr_relpath(fullpath):
    return lgetxattr(fullpath,'user.mergerfs.relpath')


def exists(base,rel,name):
    fullpath = os.path.join(base,rel,name)
    return os.path.lexists(fullpath)


def mergerfs_all_basepaths(fullpath,relpath):
    attr = xattr_allpaths(fullpath)
    if not attr:
        dirname  = os.path.dirname(fullpath)
        basename = os.path.basename(fullpath)
        attr     = xattr_allpaths(dirname)
        attr     = attr.split('\0')
        attr     = [os.path.join(path,basename)
                    for path in attr
                    if os.path.lexists(os.path.join(path,basename))]
    else:
        attr = attr.split('\0')
    return [x[:-len(relpath)].rstrip('/') for x in attr]


def mergerfs_basepath(fullpath):
    attr = xattr_basepath(fullpath)
    if not attr:
        dirname  = os.path.dirname(fullpath)
        basename = os.path.basename(fullpath)
        attr     = xattr_allpaths(dirname)
        attr     = attr.split('\0')
        for path in attr:
            fullpath = os.path.join(path,basename)
            if os.path.lexists(fullpath):
                relpath = xattr_relpath(dirname)
                return path[:-len(relpath)].rstrip('/')
    return attr


def mergerfs_relpath(fullpath):
    attr = xattr_relpath(fullpath)
    if not attr:
        dirname  = os.path.dirname(fullpath)
        basename = os.path.basename(fullpath)
        attr     = xattr_relpath(dirname)
        attr     = os.path.join(attr,basename)
    return attr.lstrip('/')


def newest_dupfun(default_basepath,relpath,basepaths):
    sts = dict([(f,os.lstat(os.path.join(f,relpath))) for f in basepaths])

    mtime = sts[basepaths[0]].st_mtime
    if not all([st.st_mtime == mtime for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_mtime,reverse=True)[0]

    ctime = sts[basepaths[0]].st_ctime
    if not all([st.st_ctime == ctime for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_ctime,reverse=True)[0]

    return default_basepath


def oldest_dupfun(default_basepath,relpath,basepaths):
    sts = dict([(f,os.lstat(os.path.join(f,relpath))) for f in basepaths])

    mtime = sts[basepaths[0]].st_mtime
    if not all([st.st_mtime == mtime for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_mtime,reverse=False)[0]

    ctime = sts[basepaths[0]].st_ctime
    if not all([st.st_ctime == ctime for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_ctime,reverse=False)[0]

    return default_basepath


def largest_dupfun(default_basepath,relpath,basepaths):
    sts = dict([(f,os.lstat(os.path.join(f,relpath))) for f in basepaths])

    size = sts[basepaths[0]].st_size
    if not all([st.st_size == size for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_size,reverse=True)[0]

    return default_basepath


def smallest_dupfun(default_basepath,relpath,basepaths):
    sts = dict([(f,os.lstat(os.path.join(f,relpath))) for f in basepaths])

    size = sts[basepaths[0]].st_size
    if not all([st.st_size == size for st in sts.values()]):
        return sorted(sts,key=lambda x: sts.get(x).st_size,reverse=False)[0]

    return default_basepath


def mergerfs_dupfun(default_basepath,relpath,basepaths):
    return default_basepath


def getdupfun(name):
    funs = {'newest': newest_dupfun,
            'oldest': oldest_dupfun,
            'smallest': smallest_dupfun,
            'largest': largest_dupfun,
            'mergerfs': mergerfs_dupfun}
    return funs[name]


def get_file_stat(fullpath):
    filestat = os.lstat(fullpath)
    return dict(zip('mode ino dev nlink uid gid size atime mtime ctime'.split(), filestat))


def get_file_digest(fullpath):
    _hash = hashlib.blake2b()
    blocksize = 65536
    stat = os.lstat(fullpath)
    with open(fullpath,'rb') as f:
        # digest = hashlib.file_digest(f, "sha256")
        buf = f.read(blocksize)
        while buf:
            _hash.update(buf)
            buf = f.read(blocksize)
    # revert atime, mtime do povodneho stavu pred ratanim digestu (musia sa obe reserovat)
    os.utime(fullpath, times=(stat.st_atime, stat.st_mtime))
    return '{}:{}'.format(_hash.name,_hash.hexdigest())


def get_different_stats(srcsts,tgtsts,fields):
    dif = dict(srcsts.items() - tgtsts.items())
    return [k for k in dif if k in fields]


def equal_stats(srcsts,tgtsts,fields):
    return True if len(get_different_stats(srcsts,tgtsts,fields)) == 0 else False


def get_file_stat_differences(srcpath,tgtpath,relpath,diffields):
    if (srcpath == tgtpath):
        return []
    srcsts = get_file_stat(os.path.join(srcpath,relpath))
    tgtsts = get_file_stat(os.path.join(tgtpath,relpath))
    return get_different_stats(srcsts,tgtsts,diffields)


def equal_files_stats(srcpath,tgtpath,relpath,diffields):
    return True if len(get_file_stat_differences(srcpath,tgtpath,relpath,diffields)) == 0 else False


def stats_content_updated(difs):
    return True if 'mtime' in difs or 'size' in difs else False


def build_existing(fileinfo,branches,caches):
    cache_list = []
    branch_list = []
    for path in fileinfo['branches'].keys():
        if path in branches:
            branch_list.append(path)
        elif path in caches:
            cache_list.append(path)
    # most free space to lowest free space - if pruning happens, prone from LFS disk
    cache_list = sorted(cache_list, key=caches.get, reverse=True)
    branch_list = sorted(branch_list, key=branches.get, reverse=True)
    return (cache_list+branch_list, cache_list, branch_list)


def files_update_new(branches,fileinfo,srcmount,relpathfile,copies,existing,count,prune,commands,logtext=""):
    i = 0
    for tgtmount in existing:
        if prune and i >= count:
            break
        copies.append(tgtmount)
        i += 1
        if fileinfo['branches'][tgtmount]['state'] > 0:
            # + new filesize - old filesize
            branches[tgtmount] += fileinfo['branches'][srcmount]['stat']['size'] - fileinfo['branches'][tgtmount]['stat']['size']
            args = build_copy_file(srcmount,tgtmount,relpathfile)
            difs = fileinfo['branches'][tgtmount]['consistencydiff']
            commands[tgtmount] = {"command":"update - overwrite {}# {}".format(logtext,', '.join(difs)),"args":args,'priority':2,'branch':tgtmount}

def files_backup_new(branches,fileinfo,srcmount,relpathfile,copies,count,commands,logtext=""):
    i = len(copies)
    for _ in range(i,count):
        for branch in sorted(branches,key=branches.get,reverse=True): # sort by most free space
            # tgtpathfile = os.path.join(branch,relpathfile)
            if branch in copies: # or os.path.exists(tgtpathfile): toto by tu nemalo byt pokial sa pred _backup vola _update
                continue
            copies.append(branch)
            branches[branch] -= fileinfo['branches'][srcmount]['stat']['size']
            args = build_copy_file(srcmount,branch,relpathfile)
            commands[branch] = {"command":"backup - copy {}".format(logtext),"args":args,'priority':1,'branch':branch}
            break

def files_prune_new(branches,fileinfo,relpathfile,copies,existing,prune,commands,logtext=""):
    if prune:
        leftovers = set(existing) - set(copies)
        for branch in leftovers:
            branches[branch] += fileinfo['branches'][branch]['stat']['size']
            tgtfile = os.path.join(branch,relpathfile)
            args = ['rm','-vf',tgtfile]
            commands[branch] = {"command":"prune - remove {}".format(logtext),"args":args,'priority':3,'branch':branch}


def print_diskdebug(srcpath, caches, existing_caches, copies_cache, branches, existing_branches, copies):
    txt_list = []
    for b in caches:
        txt_source = "=" if b == srcpath else ""
        if b in copies_cache:
            if b in existing_caches:
                txt_list.append("C{}".format(txt_source))
            else:
                txt_list.append("c{}".format(txt_source))
        else:
            if b in existing_caches:
                txt_list.append("-{}".format(txt_source))
            else:
                txt_list.append(".{}".format(txt_source))
    for b in branches:
        txt_source = "=" if b == srcpath else ""
        if b in copies:
            if b in existing_branches:
                txt_list.append("O{}".format(txt_source))
            else:
                txt_list.append("o{}".format(txt_source))
        else:
            if b in existing_branches:
                txt_list.append("-{}".format(txt_source))
            else:
                txt_list.append(".{}".format(txt_source))
    print_args(txt_list)


def dbentry_all_digests_same(entry):
    branches = entry["branches"]
    expected_value = next(iter(branches.values()))["digest"] # check for an empty dictionary first if that's possible
    all_equal = all(value["digest"] == expected_value for value in branches.values())
    return all_equal


def dbentry_get_stat_differences(entry,fields):
    # srcsts = get_file_stat(os.path.join(srcpath,relpath))
    # tgtsts = get_file_stat(os.path.join(tgtpath,relpath))
    # return get_different_stats(srcsts,tgtsts,diffields)
    branches = entry["branches"]
    difs = []
    for field in fields:
        expected_value = next(iter(branches.values()))["stat"][field] # check for an empty dictionary first if that's possible
        all_equal = all(value["stat"][field] == expected_value for value in branches.values())
        if not all_equal:
            difs.append(field)
    return difs


def dbentry_all_stats_consistent(entry,fields):
    return True if len(dbentry_get_stat_differences(entry,fields)) == 0 else False


def fileinfo_create(fullpath,branches=[],add_missing=None,integrity_digest=True):
    relpath = mergerfs_relpath(fullpath)
    if not branches:
        branches = mergerfs_all_basepaths(fullpath,relpath)

    dbentry = add_missing if add_missing else {'state':0,"consistency":None,"integrity":None,"checked":None,"branches":{}}
    branches_info = dbentry['branches']
    for branch in (b for b in branches if b not in dbentry['branches']): # skip already existing branches
        # if add_missing:
        #     console.log(branch)
        filepath = os.path.join(branch,relpath)
        branches_info[branch] = {"stat":get_file_stat(filepath),'state':0,'digest':None, 'consistencydiff':[] }
        if integrity_digest:
            branches_info[branch]["digest"] = get_file_digest(filepath)
    return dbentry


def create_dbentry(fullpath,digests=None,branches=None,integrity_digest=True):
    relpath = mergerfs_relpath(fullpath)
    existing = mergerfs_all_basepaths(fullpath,relpath)
    if branches:
        existing = list(set(existing) & set(branches))
    branches_digest = {}
    dbentry = {'state':255,"consistency":None,"integrity":None,"checked":None,"branches":branches_digest}
    for branch in existing:
        filepath = os.path.join(branch,relpath)
        branches_digest[branch] = {"stat":get_file_stat(filepath),'state':255,'digest':None, 'consistencydiff':[] }
        if integrity_digest:
            if digests and (branch in digests):
                # print("create_dbentry - using cached digest for branch {}".format(branch))
                branches_digest[branch]["digest"] = digests[branch]
            else:
                branches_digest[branch]["digest"] = get_file_digest(filepath)
    return dbentry


def xor_two_str(a,b):
    return ''.join([hex(ord(a[i%len(a)]) ^ ord(b[i%(len(b))]))[2:] for i in range(max(len(a), len(b)))])


def dbentry_check_consistency_obsolete(dbentry,diffields):
    state = dbentry["consistency"]
    if not dbentry_all_stats_consistent(dbentry,diffields):
        state = state[:0] + 'X' + state[1:]
        # print(f"    create_dbentry diff stats : {dbentry_get_stat_differences(dbentry,diffields)}")
        statdif = dbentry_get_stat_differences(dbentry,diffields)
        if stats_content_updated(statdif):
            state = state[:1] + 'X' + state[2:]
        else:
            state = state[:1] + 'O' + state[2:]
    else:
        state = state[:0] + 'OO' + state[2:]

    if not dbentry_all_digests_same(dbentry):
        state = state[:2] + 'X' + state[3:]
        # print(f"    create_dbentry diff digests")
    else:
        state = state[:2] + 'O' + state[3:]
    dbentry["consistency"] = state

    for branchdata in dbentry["branches"].values():
        branchdata["consistency"] = state

    return (state == 'OOO')


def dbentry_check_integrity_obsolete(newentry,oldentry,diffields):
    newentry['integrity'] = []

    oldbranches = oldentry["branches"]
    newbranches = newentry["branches"]

    for newbranch,newbranchvalues in newbranches.items():

        if newbranch in oldbranches:
            branchintegrity = '--'
            oldbranchvalues = oldbranches[newbranch]
            statdif = get_different_stats(newbranchvalues['stat'],oldbranchvalues['stat'],diffields)
            if stats_content_updated(statdif):
                # content zmeneny (mtime alebo size). digest nesied nemusi
                branchintegrity = 'XX'
            elif len(statdif) > 0:
                # inak sa zmenili iba file stats (mode, ...)
                branchintegrity = 'XO'
            else:
                # alebo je to presne to iste
                branchintegrity = 'OO'
            # console.log(f"{newbranch} - digest diff {xor_two_str(newbranchvalues['digest'],oldbranchvalues['digest'])}")
            if newbranchvalues['digest'] != oldbranchvalues['digest']:
                # ak nebol content zmeny a nesedi digest tak mame ERROR na disku
                branchintegrity += 'X'
            else:
                branchintegrity += 'O'

        else:
            branchintegrity = 'OOO'

        newbranchvalues['integrity'] = branchintegrity
        newentry['integrity'].append(branchintegrity)

        if branchintegrity == 'OOO':
            console.log(f"{newbranch} - files are same")
        elif branchintegrity == 'XOO':
            console.log(f"{newbranch} - stat changed {statdif}")
        elif branchintegrity == 'XXO'or branchintegrity == 'XXX':
            console.log(f"{newbranch} - content changed {statdif}")
        elif branchintegrity == 'OOX' or branchintegrity == 'XOX':
            console.log(f"{newbranch} - [red]DIGEST MISMATCH ON UNCHANGED FILE!")
        else:
            console.log(f"{newbranch} - [yellow]UNDETERMINED STATE!")


def dbentry_log(entry,diffields):
    txt = ["stat","content(stat)","content(digest)"]
    result = {'consistency':[],'integrity':[]}
    code = entry['state']
    codes = []
    for i in range(0,3):
        if (code >> i) & 1 == 1: pass #codes.append(txt[i]+" OK")
        else: codes.append("[red]"+txt[i]+"[/red]") #codes.append(txt[i]+" [red]X[/red]")
    result['consistency'] = codes
    codes = []
    for i in range(4,7):
        if (code >> i) & 1 == 1: pass #codes.append(txt[i-4]+" OK")
        else: codes.append("[red]"+txt[i-4]+"[/red]") #codes.append(txt[i-4]+" [red]X[/red]")
    result['integrity'] = codes
    return result


def create_status_code(a,b,diffields,startbit):
    state = 0
    statdif = get_different_stats(a['stat'],b['stat'],diffields)
    if len(statdif) > 0: # aspon jeden stat z diffiels sa odlisuje
        state |= 2**0
    if stats_content_updated(statdif): # zmenil sa content (size alebo mtime stat)
        state |= 2**1
    if a['digest'] != b['digest']: # zmenil sa difest
        state |= 2**2
        statdif.append('digest')
    return (state << startbit,statdif)


def fileinfo_check_consistency(fileinfo,srcpath,diffields,log=False):
    srcbranchvalues = fileinfo["branches"][srcpath]
    for branchpath,branchvalues in fileinfo["branches"].items():
        branchstate,diff = create_status_code(branchvalues,srcbranchvalues,diffields,0)
        branchvalues['state'] |= branchstate
        fileinfo['state'] |= branchstate
        branchvalues['consistencydiff'] = diff
        if log:
            console.log(f"{branchpath} : {branchstate} - consistency diff {diff}")


def fileinfo_check_integrity(fileinfo_new,fileinfo_old,diffields,log=False):
    oldbranches = fileinfo_old["branches"]
    for newbranchpath,newbranchvalues in fileinfo_new["branches"].items():
        if newbranchpath in oldbranches:
            branchstate,diff = create_status_code(newbranchvalues,oldbranches[newbranchpath],diffields,4)
            newbranchvalues['state'] |= branchstate
            fileinfo_new['state'] |= branchstate
            newbranchvalues['integritydiff'] = diff
            if log:
                console.log(f"{newbranchpath} : {branchstate} - integrity diff {diff}")


def integrity_check_file(integritydb,fullpath,srcpath,diffields,update=True,digests=None): # if branch, then check only provided branch
    relpath = mergerfs_relpath(fullpath)

    oldentry = integritydb[relpath]
    newentry = create_dbentry(fullpath,digests)

    # dbentry_check_consistency_obsolete(newentry,diffields)
    fileinfo_check_consistency(newentry,srcpath,diffields)
    # dbentry_check_integrity_obsolete(newentry,oldentry,diffields)
    fileinfo_check_integrity(newentry,oldentry,diffields)

    # console.log("INTEGRITY DB - CHECK FILE : {}".format(fullpath))
    # console.log(f"Consistency {newentry['consistency']}")
    # console.log(f"Integrity {newentry['integrity']}")
    # console.log(f"State {newentry['state']}")
    console.log(f"State {dbentry_log(newentry,diffields)}")

    # console.print(oldentry)
    # console.print(newentry)

    allsame = True if newentry["integrity"] == 'OO' else False
    allok = True
    # for newbranch, newbranchdata in newbranches.items():
    #     if oldbranches[newbranch]:
    #         if oldbranches[newbranch]["digest"] != newbranchdata["digest"]:
    #             allok &= False
    #             oldbranches[newbranch]["state"] = 'F'
    #         elif not allsame:
    #             oldbranches[newbranch]["state"] = 'M'
    #         else:
    #             oldbranches[newbranch]["state"] = 'C'

    # oldentry["time"] = time.time()
    # if not allok:
    #     oldentry["state"] = 'F'
    # elif not allsame:
    #     oldentry["state"] = 'M'
    # elif update:
    #     integritydb[relpath] = newentry
    # else:
    #     oldentry["state"] = 'C'

    # if not (allsame and allok):
    #     print(integritydb[relpath])

    return allok


def integrity_add_file(integritydb,fullpath,srcpath,diffields,digests=None):
    relpath = mergerfs_relpath(fullpath)
    isnew = not (relpath in integritydb)
    integritydb[relpath] = create_dbentry(fullpath,digests)
    fileinfo_check_consistency(integritydb[relpath],srcpath,diffields)

    console.log(f"State {dbentry_log(integritydb[relpath],diffields)}")

    if isnew:
        print("    integrity_add_file - ADD : {}".format(integritydb[relpath]["state"]))
    else:
        print("    integrity_add_file - UPDATE : {}".format(integritydb[relpath]["state"]))
    return True


def integrity_read_db(dbpath):
    db = {}
    try:
        with gzip.open(dbpath,'rb') as f:
            fcntl.flock(f,fcntl.LOCK_EX)
            db = pickle.loads(f.read())
            fcntl.flock(f,fcntl.LOCK_UN)
    except IOError as e:
        if e.errno != errno.ENOENT:
            raise
        else:
            print("Integrity database not found, creating new ...")
            return {}
    return db

def integrity_write_db(dbpath,db):
    try:
        signal.signal(signal.SIGINT,signal.SIG_IGN)
        with gzip.open(dbpath,'wb') as fw:
            fcntl.flock(fw,fcntl.LOCK_EX)
            fw.write(pickle.dumps(db))
            fcntl.flock(fw,fcntl.LOCK_UN)
    except Exception as e:
        msg = 'Error writing scorch DB: {} - {}'.format(dbpath,e)
        print(msg,file=sys.stderr)
    finally:
        signal.signal(signal.SIGINT,signal.SIG_DFL)


def get_relevant_skills(item):
    """Get the sum of Python and JavaScript skill"""
    skills = item[1]["skills"]

    # Return default value that is equivalent to no skill
    return skills.get("python", 0) + skills.get("js", 0)


def main():
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer,
                                  encoding='utf8',
                                  errors='backslashreplace',
                                  line_buffering=True)
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer,
                                  encoding='utf8',
                                  errors='backslashreplace',
                                  line_buffering=True)

    parser = buildargparser()
    args = parser.parse_args()

    if args.help or not args.dir:
        print_help()
        sys.exit(0)

    args.dir = os.path.realpath(args.dir)

    if not ismergerfs(args.dir):
        print("%s is not a mergerfs mount" % args.dir)
        sys.exit(1)

    prune = args.prune
    execute = args.execute
    diff = args.diff.split(',')
    includes = ['*'] if not args.include else args.include
    excludes = args.exclude
    excludes_cache = args.excludecache
    dupfun = getdupfun(args.dup)
    ctrlfile = mergerfs_control_file(args.dir)
    branches, caches = build_branches_freespace(mergerfs_branches(ctrlfile), args.cache)
    count_branch = min(args.count,len(branches))
    count_cache = min(1,len(caches)) # 1 if we have same args.cache else 0
    cachetime = time.time() - (2 * 24 * 60 * 60)

    try:
        '''''
        integritydb = integrity_read_db("/tmp/mergerfs.integrity.gz")
        '''''
        integritydb = {}
        # print(branches)
        # print(caches)

        for (dirpath,dirnames,filenames) in os.walk(args.dir):
            if match(dirpath, excludes) or not match(dirpath, includes):
                continue

            # count_cache = 0 if folder current folder excluded from caching or no cache disk defined else 1
            count_cache = 0 if match(dirpath,excludes_cache) else min(1,len(caches))

            for filename in filenames:

                if match(filename,excludes) or not match(filename,includes):
                    continue

                fullpath = os.path.join(dirpath,filename)
                # console.print(f"[bright_red]{fullpath}[/bright_red]")

                basepath = mergerfs_basepath(fullpath)
                relpath  = mergerfs_relpath(fullpath)
                fileinfo = fileinfo_create(fullpath,integrity_digest=False) # zatial digesty nepotrebujeme, kym neviem, ci budu nejake commands
                existing, existing_caches, existing_branches = build_existing(fileinfo,branches,caches)
                srcpath = dupfun(basepath,relpath,existing) # source disk path, depends on args.dup (defaults to "mergerfs policy")
                srcfile  = os.path.join(srcpath,relpath)

                copies_cache_new = []
                copies_new = []
                commands_new = {}
                fileinfo_check_consistency(fileinfo,srcpath,diff)
                # CACHES
                if count_cache > 0 and match(filename,excludes_cache): # set count_cache = 0 if we are caching, but filename is excluded from caching
                    count_cache = 0
                files_update_new(caches,fileinfo,srcpath,relpath,copies_cache_new,existing_caches,count_cache,prune,commands_new,"cache")
                if (fileinfo['branches'][srcpath]['stat']['atime'] > cachetime):
                    files_backup_new(caches,fileinfo,srcpath,relpath,copies_cache_new,count_cache,commands_new,logtext="to cache")
                files_prune_new(caches,fileinfo,relpath,copies_cache_new,existing_caches,prune,commands_new,logtext="cached")
                # BRANCHES
                files_update_new(branches,fileinfo,srcpath,relpath,copies_new,existing_branches,count_branch,prune,commands_new)
                files_backup_new(branches,fileinfo,srcpath,relpath,copies_new,count_branch,commands_new)
                files_prune_new(branches,fileinfo,relpath,copies_new,existing_branches,prune,commands_new)

                if commands_new:
                    existing_str = f'{len(existing_caches)}{len(existing_branches)}'.replace("0","-")
                    copies_str = f'{len(copies_cache_new)}{len(copies_new)}'.replace("0","-")
                    console.print(f"[cyan]{srcfile} [white][{existing_str}]>[{copies_str}]")

                    fileinfo = fileinfo_create(fullpath,branches=[srcpath])
                    # 3. porovnaj s databazou ak tam je ak je ok, tak mozme spustit commandy
                    # dbentry_check_integrity(fileinfo,,diff)
                    # todo: check source file integrity and continue only if ok else throw error in HA!!! disk
                    # ma to zmysel ak je zmena iba metadatach (ctime) alebo mazani
                    # console.log(fileinfo)

                    # execute commands
                    copies_nochange = copies_cache_new+copies_new
                    copies_nochange.remove(srcpath)
                    for branchname,branchdata in sorted(commands_new.items(), key=lambda data: data[1]["priority"]):
                        console.print(f"[bright_black](p {branchdata['priority']})[/bright_black] {branchname} : {branchdata['command']}")
                        if branchdata['branch'] in copies_nochange:
                            copies_nochange.remove(branchdata['branch'])
                        if execute:
                            execute_cmd(branchdata["args"])

                    console.log(copies_nochange)

                    # add missing bramcjes, not source only
                    fileinfo = fileinfo_create(fullpath,add_missing=fileinfo)
                    fileinfo_check_consistency(fileinfo,srcpath,diff)
                    fileinfo_check_integrity(fileinfo,fileinfo,diff) # compare with itself = force true as everything would be same as it was just created
                    # 4. branche, ktore sa nemenili (copies_nochange) by sme mali takisto skontrolovat s databazou
                    # 5. zapis do databazy
                    # todo

                    # if relpath in integritydb:
                    #     integrity_check_file(integritydb,fullpath,srcpath,diff)
                    # else:
                    #     integrity_add_file(integritydb,fullpath,srcpath,diff)


                '''
                if commands:
                    existing_str = f'{len(existing_caches)}{len(existing_branches)}'.replace("0","-")
                    copies_str = f'{len(copies_cache)}{len(copies)}'.replace("0","-")
                    console.print(f"[green]{srcfile} [white][{existing_str}]>[{copies_str}]")
                    # console.print(f"{commands}")
                    # console.print(f"{existing_caches} -> {copies_cache}")
                    # console.print(f"{existing_branches} -> {copies}")
                    for branchname,branchdata in sorted(commands.items(), key=lambda data: data[1]["priority"]):
                        console.print(f"[bright_black](p {branchdata['priority']})[/bright_black] {branchname} : {branchdata['command']}")
                        if execute:
                            execute_cmd(branchdata["args"])

                '''

                """
                else:
                    if relpath in integritydb:
                        integrity_check_file(integritydb,fullpath,srcpath,diff)
                    else:
                        integrity_add_file(integritydb,fullpath,diff)

                    # nejak pridat cekovanie pred komandom
                    allok = True
                    srcdigest = get_file_digest(srcfile)§
                    if (relpath in integritydb) and (srcpath in integritydb[relpath]['branches']):
                        allok = integrity_check_file_branch(integritydb,fullpath,diff,branch=srcpath,digests=srcdigest)
                        print(allok)
                    if allok:
                        for branchdata in commands.values(): # commands[branch] = branchdata
                            execute_cmd(branchdata["args"])
                            pass
                        # znovu pridame aj ked existuje, kedze sa pomenili data, metadata
                        integrity_add_file(integritydb,fullpath,diff,digests={srcpath:srcdigest})
                    """
                # ADD IF NOT IN DATAINTEGIRTY DATABASE FOR ALL CREATED BRANCHES


                # print("---")
                # print("basepath: ",srcpath)
                # print("srcpath: ",srcpath)
                # print("srcfile: ",srcfile)
                # print_diskdebug(srcpath, caches, existing_caches, copies_cache, branches, existing_branches, copies)
                # print('existing_caches: {}'.format(existing_caches))
                # print('existing_branches: {}'.format(existing_branches))
                # print("copies_cache = {}".format(copies_cache))
                # print("copies = {}".format(copies))
        #pprint(integritydb)
        # print(json.dumps(integritydb, indent=4, sort_keys=True))
        # integrity_write_db("/tmp/mergerfs.integrity.gz",integritydb)

# U - unknown? new?
# O - file changed (updated)
# C - file changed (not updated)
# F - file not changed, digest failed
# O - file not changed, digest ok, but some other file stats changed (updated)
# O - file not changed, digest ok, no changes at all or some other file stats changed (not updated)

# todo
# zmazavanie v cache aj backing dat uplne nakoniec

    except KeyboardInterrupt:
        print("exiting: CTRL-C pressed")
    except BrokenPipeError:
        pass

    sys.exit(0)


if __name__ == "__main__":
   main()
